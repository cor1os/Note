#"机器人防火墙"：人机识别在应用安全及风控领域的一点实践

美剧 WestWorld 第二集里有个场景十分有意思：游客来到西部世界公园，遇到了一个漂亮的女接待员，但无法区分对方是否是人类，于是产生了如下对话：

Guest: "Are you real?"  -- Are you a Human or a Host?

Host: "If you can't tell, Does it matter?"

可以说科技尤其人工智能的发展，机器人已经足以模拟人类达到以假乱真的地步，不过今天要谈的并不是人工智能、不是机器学习。诚然AI可以在人机识别及诸多方面做得很好（假设有足够的训练样本），但它未必是最好、也不是唯一的选择，至少在我所说的当前场景中。

作为科普和背景铺垫，感兴趣的可以检索：

验证码的前世今生（前世篇）

验证码的前世今生（今生篇）

直入主题，安全领域中爬虫、暴力破解、扫描、DDoS等攻击行为都是通过工具发起，操纵者预设指令而工具去自动执行直到任务完成或出于某种原因中断运行。传统安全机制而言，WAF、IPS等这类产品的确在一定程度能够保护用户免受攻击，但鉴于它们都依赖于特征和规则库又有很多限制。

时代在变化，攻击者也在不断尝试新的机会和场景，比如竞争性爬虫、撞库、薅羊毛、虚假注册、灌水刷评论等等。全局分析时，我们可以认为这些操作都是恶意且应被阻断的，然而单独来看每一个操作又是合法且正常的，没有任何攻击特征，WAF解决不了，代码层面做诸多做限制也解决不了。

没有攻击特征，不代表无法识别，可以肯定的是这些针对业务发起的"定制"攻击绝大多数来自于自动化工具，或者说机器人。然而问题难就难在这里，怎么区分一个请求的发起者是人还是机器？

业务方会做很多事情限制非法机器人的访问，最常见的是使用图形验证码、短信验证码。验证码机制早期确实很有效果，但现在也遭遇着巨大的挑战，比如国内打码平台的出现及使用。另一方面，验证码会对用户体验造成较大伤害，企业不得不做一个安全与使用方便的折中，于是出现了滑块验证码。相比扭曲的图形验证码，理论上基于用户行为的滑块验证码无论从安全级别还是易用性都可以有巨大提升，然而它只是安全防护机制其中的一道，存在被突破的可能，而且已经被突破。

实力较好的公司有人专门负责风险控制（简称风控），国内目前也有很多风控服务和产品的提供商，大多是从多维度的数据层面（如过往操作记录、设备指纹等）为某一个"用户"的操作提供信誉评分，确定其是否合法，例如转账支付是否为本人操作。阿里云是这方面的杰出代表，不仅数据层面而且在用户行为识别方面做了不少工作。不过其在风控方面的成就可复制性并不高，首先不是每家公司都有能力做好海量大数据分析，其次用户行为识别需要有很强的前端技术能力，而这种能力又会以数据的形式转化为大数据的分析能力。

下面介绍一下我们在用户行为特别是人机识别上的一些实践，以下均以Web应用为例说明。

对于Web应用而言，（视情况而定）本身就存在较多敏感信息，比如页面内容、JS源代码、后续链接、表单信息等，它们构成了一个应用的运行逻辑。只要能判断这个逻辑，应用行为就是可以预测的，这也是爬虫、扫描器等工具运作的基本原理。那如果我可以对应用页面的源代码进行混淆或加密，是否能使工具失去作用呢？答案是在一定程度上可以，但这里面临几个问题。

首先，页面中包含各种标签、表单、链接、JS等信息，哪些信息需要混淆需要明确。最重要的是，虽然可以使用算法进行混淆，但是把混淆后的代码还原让浏览器可以解析以保证应用可正常访问则要难得多。正是因为还原混淆代码的难度大，很少有应用采用这种保护方式。

其次，如果算法固定且考虑到性能问题，加密信息和算法可以被逆向还原，稍微做一些定制开发就可以使工具再次自动运行。如果有多种混淆算法，如何决定在某一时刻使用哪种算法？

上述两个问题已经超越了本人能回答的范畴，但可以讲一下我司已经实现的效果：

1. 页面的表单、URL、JS代码全部使用专有算法封装，可理解为加密，其余内容不做处理；

2. 算法是动态变换的，每次请求譬如刷新页面都会随机使用不同算法，可使用算法数40亿以上；

3. 所有封装通过JS实现，用于加密页面的JS本身被混淆且没有逆袭价值；

4. 被保护页面具备防止调试和虚拟执行的能力；

5. 正常用户通过浏览器进行访问不受任何影响；

根据Distil公司的统计，54%的自动化工具不具备Javascript解析能力，这意味着超过一半的工具将无法使用，无需任何规则即可有效阻拦大量机器，此为人机识别第一步。

虽然商用扫描工具很多都具备JS解析能力，但对于这类机器行为仍有办法识别——动态验证。简单来说，动态验证会校验JS的实际运行环境、获取设备指纹，以及采集客户端行为——例如是否有键盘鼠标操作及在页面停留时间等。通过多维度数据分析可以区分操作的是人还是机器，此为人机识别第二步。

对于提交的表单信息，也可以选择对其进行混淆，即便通过中间人攻击截获了请求，看到的都是乱码，无法窃取信息、无法伪造、无法篡改。由于算法是动态的，同样不担心被逆向或破解。这一步本身不是用于识别工具行为，却可以有效防御借助工具进行的中间人攻击等行为。

动态令牌机制可以作为人机识别的第三步来看，可以有效防止请求重放等非法操作。即便不对页面进行封装，仍然会（根据上一个页面的访问）动态生成每个请求唯一的令牌，存放在Cookie中。保护模式下每个令牌都只能用一次，一旦令牌为空、重复使用、或者被篡改，请求就会被视为无效请求而拦截。

综合以上几个过程，从简单到高级的Burp、爬虫、扫描器、PhantomJS等机器行为都能被有效识别，理论误判概率％0。最重要的是，做这么多事情无须应用改一行哪怕半行代码，客户端无需做任何更改，各大主流及非主流浏览器访问均不受任何影响。

我们使用的方案除了人际识别当然还有更多的能力，这里不做描述，感兴趣的可以联系我。

实际上，如果能在非法请求到达后台之前能够对其进行识别并采取阻断措施，带来的效益除了减缓攻击，还可以节约带宽、服务器资源、过滤大量噪音和垃圾数据。更重要的是，我们使用的是一种全程防护的方式却又很轻量级，变被动为主动，先发制人。

更多信息请见：https://www.riversecurity.com.cn | 私聊

Referer: [https://www.cnblogs.com/r00tgrok/p/RiverSecurity-Bots-FireWall.html](https://www.cnblogs.com/r00tgrok/p/RiverSecurity-Bots-FireWall.html)